{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q torchvision pandas\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charts.common.dataset import LabeledImage\n",
    "from charts.common.timer import Timer\n",
    "import charts.pytorch.color_regression as cr\n",
    "import charts.pytorch.utils as utils\n",
    "from charts.pytorch.utils import Experiment, num_trainable_parameters, is_google_colab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch_lr_finder\n",
    "import timm\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from icecream import ic\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "display(f\"Use CUDA: {use_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = cr.ImagePreprocessor(device, target_size=128)\n",
    "\n",
    "dataset_path = Path(\"/content/datasets/drawings\") if is_google_colab() else Path('../../generated/drawings')\n",
    "\n",
    "dataset = cr.ColorRegressionImageDataset(dataset_path, preprocessor)\n",
    "n_train = max(int(len(dataset) * 0.5), 1)\n",
    "n_val = len(dataset) - n_train\n",
    "# train_dataset, val_dataset = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_indices = range(0, n_train)\n",
    "val_indices = range(n_train, len(dataset))\n",
    "\n",
    "small_subset = not is_google_colab()\n",
    "if small_subset:\n",
    "    N = 16\n",
    "    train_indices = random.sample(train_indices, N)\n",
    "    val_indices = random.sample(val_indices, N)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices, generator=generator)\n",
    "val_sampler = SubsetRandomSampler(val_indices, generator=generator)\n",
    "\n",
    "BATCH_SIZE=8 if is_google_colab() else 4\n",
    "\n",
    "train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\n",
    "val_dataloader = DataLoader(dataset, sampler=val_sampler, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\n",
    "\n",
    "monitored_train_samples = random.sample(train_indices, 5)\n",
    "monitored_val_samples = random.sample(val_indices, 5)\n",
    "# monitored_sample = dataset[0]\n",
    "# monitored_sample_inputs = torch.unsqueeze(monitored_sample[0], dim=0)\n",
    "# monitored_samples_json = ["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, name): self.name = name\n",
    "    def create_net(self): return None\n",
    "    def create_optimizer(self, net): return None\n",
    "    def create_scheduler(self, optimizer, frozen, steps_per_epoch, total_epochs): return None\n",
    "    def get_hyperparams(self): return dict(name=self.name)\n",
    "\n",
    "net = None\n",
    "\n",
    "def run_xp_config (xp: Experiment, config: Config, frozen_epochs: int, total_epochs: int):\n",
    "    torch.cuda.empty_cache()\n",
    "    global net # Make sure that we keep the last net to play with it after.    \n",
    "    \n",
    "    # Make sure that we release as much memory as possible\n",
    "    net = None\n",
    "    utils.clear_gpu_memory()\n",
    "\n",
    "    net = config.create_net()\n",
    "    net.to(device)\n",
    "    optimizer = config.create_optimizer(net)\n",
    "\n",
    "    xp.prepare (config.name, net, optimizer, device, dataset[0][0].unsqueeze(0).to(device))\n",
    "    xp.writer.add_text(\"Model Complexity\", \", \".join(get_model_complexity_info(net, (3, 128, 128), as_strings=True, print_per_layer_stat=False, verbose=False)), global_step=None, walltime=None)    \n",
    "     \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    val_accuracy = 0.0\n",
    "    training_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    def train (first_epoch, end_epoch, optimizer, scheduler):\n",
    "        pbar = tqdm(range(first_epoch, end_epoch))\n",
    "        for epoch in pbar:  # loop over the dataset multiple times\n",
    "            nonlocal training_loss, val_loss, val_accuracy\n",
    "            net.train()\n",
    "            cumulated_training_loss = 0.0\n",
    "            tstart = time.time()\n",
    "            \n",
    "            # batch_bar = tqdm(train_dataloader, leave=False)\n",
    "            for i, data in enumerate(train_dataloader):\n",
    "                inputs, labels, json_files = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_loss = loss.item()\n",
    "                # xp.writer.add_scalar(\"Single Batch Loss\", batch_loss, epoch)\n",
    "\n",
    "                cumulated_training_loss += batch_loss\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step()            \n",
    "\n",
    "            # Very important for batch norm layers.\n",
    "            net.eval()\n",
    "\n",
    "            def evaluate_images_at_indices(indices):                \n",
    "                inputs = []\n",
    "                outputs = []\n",
    "                targets = []\n",
    "                for idx in indices:\n",
    "                    input, target = [x.to(device) for x in dataset[idx][:2]]\n",
    "                    output = net(input.unsqueeze(0)).squeeze(0)\n",
    "                    inputs.append(preprocessor.denormalize_and_clip_as_tensor(input.detach().cpu()))\n",
    "                    outputs.append(preprocessor.denormalize_and_clip_as_tensor(output.detach().cpu()))\n",
    "                    targets.append(preprocessor.denormalize_and_clip_as_tensor(target.detach().cpu()))\n",
    "                return torch.cat([torch.cat(outputs, dim=2), torch.cat(targets, dim=2), torch.cat(inputs, dim=2)], dim=1)\n",
    "\n",
    "            results_train = evaluate_images_at_indices(monitored_train_samples)\n",
    "            xp.writer.add_image(\"Train Samples\", results_train, epoch)\n",
    "\n",
    "            results_val = evaluate_images_at_indices(monitored_val_samples)\n",
    "            xp.writer.add_image(\"Val Samples\", results_val, epoch)\n",
    "\n",
    "            training_loss = cumulated_training_loss / len(train_dataloader)\n",
    "            xp.writer.add_scalar(\"Training Loss\", training_loss, epoch)\n",
    "            \n",
    "            val_loss = cr.compute_average_loss (val_dataloader, net, criterion, device)\n",
    "            xp.writer.add_scalar(\"Validation Loss\", val_loss, epoch)\n",
    "\n",
    "            val_accuracy = cr.compute_accuracy (val_dataloader, net, criterion, device)\n",
    "            xp.writer.add_scalar(\"Validation Accuracy\", val_accuracy, epoch)\n",
    "\n",
    "            elapsedSecs = (time.time() - tstart)\n",
    "            xp.writer.add_scalar(\"Elapsed Time (s)\", elapsedSecs, epoch)\n",
    "            # print(f\"[{epoch}] [TRAIN_LOSS={training_loss:.4f}] [VAL_LOSS={val_loss:.4f}] [{elapsedSecs:.1f}s]\")\n",
    "            \n",
    "            xp.writer.add_histogram(\"enc0\", net.decoder.enc0.block[3].weight, global_step=epoch)\n",
    "            xp.writer.add_histogram(\"dec0\", net.decoder.dec0.block[3].weight, global_step=epoch)\n",
    "\n",
    "            pbar.set_postfix({'train_loss': training_loss, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\n",
    "\n",
    "            if epoch % 5 == 1:\n",
    "                xp.save_checkpoint(epoch)\n",
    "\n",
    "    net.freeze_encoder()\n",
    "    ic(num_trainable_parameters(net))\n",
    "    scheduler = config.create_scheduler(optimizer, frozen=True, steps_per_epoch=len(train_dataloader), total_epochs=(frozen_epochs - xp.first_epoch))\n",
    "    train(xp.first_epoch, frozen_epochs, optimizer, scheduler)\n",
    "\n",
    "    net.unfreeze_encoder()\n",
    "    scheduler = config.create_scheduler(optimizer, frozen=False, steps_per_epoch=len(train_dataloader), total_epochs=(total_epochs - frozen_epochs))\n",
    "    ic(num_trainable_parameters(net))\n",
    "    train(frozen_epochs, total_epochs, optimizer, scheduler)\n",
    "\n",
    "    xp.finalize(hparams = config.get_hyperparams(), metrics={'hparam/train_loss': training_loss, 'hparam/val_loss': val_loss, 'hparam/accuracy': val_accuracy})\n",
    "    print('Finished Training!')\n",
    "    utils.clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training!\n",
      "=== RUNNING CONFIG unet1_adamw_1cycle_3e4_1e4 ==\n",
      "[XP] storing config data to /home/nb/Perso/DaltonLensPrivate/charts/pytorch/experiments/2022-Feb01-CR1/unet1_adamw_1cycle_3e4_1e4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_trainable_parameters(net): 18331523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4496acd2b9924f9ab6bddfed6ecc930b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_trainable_parameters(net): 29508035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b5b61a6b954e1395d98b22f6eaefc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training!\n",
      "=== RUNNING CONFIG unet1_adamw_1cycle_3e4_3e4 ==\n",
      "[XP] storing config data to /home/nb/Perso/DaltonLensPrivate/charts/pytorch/experiments/2022-Feb01-CR1/unet1_adamw_1cycle_3e4_3e4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_trainable_parameters(net): 18331523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1894a7bbe99f41b4af28665de0c4d83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_trainable_parameters(net): 29508035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ce4631f90547afadf9e2802bcd7919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training!\n",
      "=== RUNNING CONFIG unet1_adamw_1cycle_1e3_1e3 ==\n",
      "[XP] storing config data to /home/nb/Perso/DaltonLensPrivate/charts/pytorch/experiments/2022-Feb01-CR1/unet1_adamw_1cycle_1e3_1e3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_trainable_parameters(net): 18331523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fc77d0f3e94d7cbdf859259334e202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_trainable_parameters(net): 29508035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef767f34f004436789f06485c382af9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ConfigUnet1Adam(Config):\n",
    "    def __init__(self, name, max_lr_frozen, max_lr_tune, one_cycle: bool = True):\n",
    "        super().__init__(name)\n",
    "        self.one_cycle = one_cycle\n",
    "        self.max_lr_frozen = max_lr_frozen\n",
    "        self.max_lr_tune = max_lr_tune\n",
    "\n",
    "    def create_net(self): return cr.RegressionNet_Unet1()\n",
    "\n",
    "    def create_optimizer(self, net): \n",
    "        return optim.Adam([\n",
    "            {'params': net.encoder.parameters(), 'lr': self.max_lr_frozen[0] },\n",
    "            {'params': net.decoder.parameters(), 'lr': self.max_lr_frozen[1] }\n",
    "        ])\n",
    "\n",
    "    def create_scheduler(self, optimizer, frozen: bool, steps_per_epoch: int, total_epochs: int):\n",
    "        if not self.one_cycle:\n",
    "            return None\n",
    "        max_lr = self.max_lr_frozen if frozen else self.max_lr_tune\n",
    "        return torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, steps_per_epoch=steps_per_epoch, epochs=total_epochs)\n",
    "\n",
    "    def get_hyperparams(self):\n",
    "        return super().get_hyperparams() | dict(\n",
    "            net='unet1',\n",
    "            opt='adam',\n",
    "            sched='1cycle' if self.one_cycle else 'none',\n",
    "            enc_lr_frozen=self.max_lr_frozen[0],\n",
    "            dec_lr_frozen=self.max_lr_frozen[1],\n",
    "            enc_lr_tune=self.max_lr_tune[0],\n",
    "            dec_lr_tune=self.max_lr_tune[1]\n",
    "        )\n",
    "\n",
    "class ConfigUnet1AdamW(ConfigUnet1Adam):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def create_optimizer(self, net): \n",
    "        return optim.AdamW([\n",
    "            {'params': net.encoder.parameters(), 'lr': self.max_lr_frozen[0] },\n",
    "            {'params': net.decoder.parameters(), 'lr': self.max_lr_frozen[1] }\n",
    "        ])\n",
    "\n",
    "    def get_hyperparams(self):\n",
    "        return super().get_hyperparams() | dict(opt='AdamW')\n",
    "\n",
    "class ConfigUnet1SGD(ConfigUnet1Adam):\n",
    "    def __init__(self, name, momentum, *args, **kwargs):\n",
    "        super().__init__(name, *args, **kwargs)\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def create_optimizer(self, net):\n",
    "        return optim.SGD([\n",
    "            {'params': net.encoder.parameters(), 'lr': self.max_lr_frozen[0] },\n",
    "            {'params': net.decoder.parameters(), 'lr': self.max_lr_frozen[1] }\n",
    "        ], momentum=self.momentum)\n",
    "\n",
    "    def get_hyperparams(self):\n",
    "        return super().get_hyperparams() | dict(opt='SGD')\n",
    "\n",
    "configs = [\n",
    "    ConfigUnet1Adam('unet1_adam_3e4', max_lr_frozen=(1e-5, 3e-4), max_lr_tune=(1e-5, 3e-4), one_cycle=False),\n",
    "\n",
    "    ConfigUnet1Adam1Cycle('unet1_adam_1cycle_1e3', max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 3e-4)),\n",
    "    ConfigUnet1Adam1Cycle('unet1_adam_1cycle_3e4', max_lr_frozen=(1e-5, 3e-4), max_lr_tune=(1e-5, 1e-4)),\n",
    "    \n",
    "    ConfigUnet1SGD1Cycle('unet1_sgd_1cycle_1e3_09', max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 3e-4), momentum=0.9),\n",
    "    ConfigUnet1SGD1Cycle('unet1_sgd_1cycle_1e3_099', max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 3e-4), momentum=0.99),\n",
    "\n",
    "    ConfigUnet1AdamW1Cycle('unet1_adamw_1cycle_1e3_3e4', max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 3e-4)),\n",
    "    ConfigUnet1AdamW1Cycle('unet1_adamw_1cycle_3e4_1e4', max_lr_frozen=(1e-5, 3e-4), max_lr_tune=(1e-5, 1e-4)),\n",
    "    ConfigUnet1AdamW1Cycle('unet1_adamw_1cycle_3e4_3e4', max_lr_frozen=(1e-5, 3e-4), max_lr_tune=(1e-5, 3e-4)),\n",
    "    ConfigUnet1AdamW1Cycle('unet1_adamw_1cycle_1e3_1e3', max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 1e-3)),    \n",
    "]\n",
    "\n",
    "xp = Experiment(\"2022-Feb01-CR1\", clear_previous_results=True, clear_top_folder=True)\n",
    "for config in configs:\n",
    "    print (f\"=== RUNNING CONFIG {config.name} ==\")\n",
    "    run_xp_config (xp, config, frozen_epochs=50, total_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_specific_checkpoint (name):\n",
    "    checkpoint = torch.load(xp.log_path / name, map_location=device)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# load_specific_checkpoint (\"checkpoint-00701.pt\")\n",
    "# torch.save (net, \"regression_unet_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input, labels, _ = next(iter(train_dataloader))\n",
    "    input, labels = [x.to(device) for x in [input, labels]]\n",
    "    output = net(input.to(device))\n",
    "    #clear_output(wait=True)\n",
    "    plt.figure()\n",
    "    plt.imshow (preprocessor.denormalize_and_clip_as_numpy(output[0]))\n",
    "    plt.figure()\n",
    "    plt.imshow (preprocessor.denormalize_and_clip_as_numpy(labels[0]))\n",
    "    plt.figure()\n",
    "    plt.imshow (preprocessor.denormalize_and_clip_as_numpy(input[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the google colab VM\n",
    "utils.stop_google_colab_vm ()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "016a8caea8d0bcbcb0f585ee40a090ed0405075dd6b5b528270a2e6b8c256090"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
