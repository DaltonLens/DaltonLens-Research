{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charts.common.dataset import LabeledImage\n",
    "from charts.common.timer import Timer\n",
    "import charts.pytorch.color_regression as cr\n",
    "import charts.pytorch.utils as utils\n",
    "from charts.pytorch.utils import Experiment, num_trainable_parameters, is_google_colab, merge_dicts\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torchvision import transforms\n",
    "import torch.profiler\n",
    "\n",
    "import torch_lr_finder\n",
    "import timm\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from icecream import ic\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Use CUDA: True'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "display(f\"Use CUDA: {use_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = cr.ImagePreprocessor(device, target_size=128)\n",
    "\n",
    "dataset_path = Path(\"/content/datasets/drawings\") if is_google_colab() else Path('../../generated/drawings')\n",
    "\n",
    "dataset = cr.ColorRegressionImageDataset(dataset_path, preprocessor)\n",
    "n_train = max(int(len(dataset) * 0.5), 1)\n",
    "n_val = len(dataset) - n_train\n",
    "# train_dataset, val_dataset = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_indices = range(0, n_train)\n",
    "val_indices = range(n_train, len(dataset))\n",
    "\n",
    "small_subset = not is_google_colab()\n",
    "if small_subset:\n",
    "    N = 16\n",
    "    train_indices = random.sample(train_indices, N)\n",
    "    val_indices = random.sample(val_indices, N)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices, generator=generator)\n",
    "val_sampler = SubsetRandomSampler(val_indices, generator=generator)\n",
    "\n",
    "DEFAULT_BATCH_SIZE=128 if is_google_colab() else 4\n",
    "WORKERS=os.cpu_count()\n",
    "# WORKERS=0\n",
    "\n",
    "monitored_train_samples = random.sample(train_indices, 5)\n",
    "monitored_val_samples = random.sample(val_indices, 5)\n",
    "# monitored_sample = dataset[0]\n",
    "# monitored_sample_inputs = torch.unsqueeze(monitored_sample[0], dim=0)\n",
    "# monitored_samples_json = ["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, name, batch_size=DEFAULT_BATCH_SIZE): \n",
    "        self.name = name\n",
    "        self.batch_size = batch_size\n",
    "    def create_net(self): return None\n",
    "    def create_optimizer(self, net): return None\n",
    "    def create_scheduler(self, optimizer, frozen, steps_per_epoch, total_epochs): return None\n",
    "    def get_hyperparams(self): return dict(name=self.name, batch=self.batch_size)\n",
    "\n",
    "net = None\n",
    "\n",
    "def run_xp_config (xp: Experiment, config: Config, frozen_epochs: int, total_epochs: int, profiler = None):\n",
    "    torch.cuda.empty_cache()\n",
    "    global net # Make sure that we keep the last net to play with it after.    \n",
    "    \n",
    "    # Make sure that we release as much memory as possible\n",
    "    net = None\n",
    "    utils.clear_gpu_memory()\n",
    "\n",
    "    net = config.create_net()\n",
    "    net.to(device)\n",
    "    optimizer = config.create_optimizer(net)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=config.batch_size, num_workers=WORKERS)\n",
    "    val_dataloader = DataLoader(dataset, sampler=val_sampler, batch_size=config.batch_size, num_workers=WORKERS)\n",
    "\n",
    "    xp.prepare (config.name, net, optimizer, device, dataset[0][0].unsqueeze(0).to(device))\n",
    "    xp.writer.add_text(\"Model Complexity\", \", \".join(get_model_complexity_info(net, (3, 128, 128), as_strings=True, print_per_layer_stat=False, verbose=False)), global_step=None, walltime=None)    \n",
    "     \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    val_accuracy = 0.0\n",
    "    training_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    def train (first_epoch, end_epoch, optimizer, scheduler):\n",
    "        pbar = tqdm(range(first_epoch, end_epoch))\n",
    "        for epoch in pbar:  # loop over the dataset multiple times\n",
    "            nonlocal training_loss, val_loss, val_accuracy\n",
    "            net.train()\n",
    "            cumulated_training_loss = 0.0\n",
    "            tstart = time.time()\n",
    "            \n",
    "            # batch_bar = tqdm(train_dataloader, leave=False)\n",
    "            for i, data in enumerate(train_dataloader):\n",
    "                inputs, labels, json_files = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_loss = loss.item()\n",
    "                # xp.writer.add_scalar(\"Single Batch Loss\", batch_loss, epoch)\n",
    "\n",
    "                cumulated_training_loss += batch_loss\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "\n",
    "                if profiler:\n",
    "                    profiler.step()\n",
    "\n",
    "            # Very important for batch norm layers.\n",
    "            net.eval()\n",
    "\n",
    "            def evaluate_images_at_indices(indices):                \n",
    "                inputs = []\n",
    "                outputs = []\n",
    "                targets = []\n",
    "                for idx in indices:\n",
    "                    input, target = [x.to(device) for x in dataset[idx][:2]]\n",
    "                    output = net(input.unsqueeze(0)).squeeze(0)\n",
    "                    inputs.append(preprocessor.denormalize_and_clip_as_tensor(input.detach().cpu()))\n",
    "                    outputs.append(preprocessor.denormalize_and_clip_as_tensor(output.detach().cpu()))\n",
    "                    targets.append(preprocessor.denormalize_and_clip_as_tensor(target.detach().cpu()))\n",
    "                return torch.cat([torch.cat(outputs, dim=2), torch.cat(targets, dim=2), torch.cat(inputs, dim=2)], dim=1)\n",
    "\n",
    "            results_train = evaluate_images_at_indices(monitored_train_samples)\n",
    "            xp.writer.add_image(\"Train Samples\", results_train, epoch)\n",
    "\n",
    "            results_val = evaluate_images_at_indices(monitored_val_samples)\n",
    "            xp.writer.add_image(\"Val Samples\", results_val, epoch)\n",
    "\n",
    "            training_loss = cumulated_training_loss / len(train_dataloader)\n",
    "            xp.writer.add_scalar(\"Training Loss\", training_loss, epoch)\n",
    "            \n",
    "            val_loss = cr.compute_average_loss (val_dataloader, net, criterion, device)\n",
    "            xp.writer.add_scalar(\"Validation Loss\", val_loss, epoch)\n",
    "\n",
    "            val_accuracy = cr.compute_accuracy (val_dataloader, net, criterion, device)\n",
    "            xp.writer.add_scalar(\"Validation Accuracy\", val_accuracy, epoch)\n",
    "\n",
    "            elapsedSecs = (time.time() - tstart)\n",
    "            xp.writer.add_scalar(\"Elapsed Time (s)\", elapsedSecs, epoch)\n",
    "            # print(f\"[{epoch}] [TRAIN_LOSS={training_loss:.4f}] [VAL_LOSS={val_loss:.4f}] [{elapsedSecs:.1f}s]\")\n",
    "            \n",
    "            xp.writer.add_histogram(\"enc0\", net.decoder.enc0.block[3].weight, global_step=epoch)\n",
    "            xp.writer.add_histogram(\"dec0\", net.decoder.dec0.block[3].weight, global_step=epoch)\n",
    "\n",
    "            pbar.set_postfix({'train_loss': training_loss, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\n",
    "\n",
    "            # if epoch % 5 == 1:\n",
    "            #     xp.save_checkpoint(epoch)\n",
    "\n",
    "        xp.save_checkpoint(end_epoch-1)\n",
    "\n",
    "    net.freeze_encoder()\n",
    "    ic(num_trainable_parameters(net))\n",
    "    scheduler = config.create_scheduler(optimizer, frozen=True, steps_per_epoch=len(train_dataloader), total_epochs=frozen_epochs)\n",
    "    train(xp.first_epoch, frozen_epochs, optimizer, scheduler)\n",
    "\n",
    "    net.unfreeze_encoder()\n",
    "    scheduler = config.create_scheduler(optimizer, frozen=False, steps_per_epoch=len(train_dataloader), total_epochs=(total_epochs - frozen_epochs))\n",
    "    ic(num_trainable_parameters(net))\n",
    "    train(frozen_epochs, total_epochs, optimizer, scheduler)\n",
    "\n",
    "    xp.finalize(hparams = config.get_hyperparams(), metrics={'hparam/train_loss': training_loss, 'hparam/val_loss': val_loss, 'hparam/accuracy': val_accuracy})\n",
    "    print('Finished Training!')\n",
    "    utils.clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XP] storing experiment data to /content/drive/MyDrive/DaltonLens-Colab/DaltonLensPrivate/charts/pytorch/experiments/2022-Feb01-CR1\n",
      "=== [1/6] RUNNING CONFIG unet1_adamw_1cycle_bn128_1e3_3e4 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XP] storing config data to /content/drive/MyDrive/DaltonLens-Colab/DaltonLensPrivate/charts/pytorch/experiments/2022-Feb01-CR1/unet1_adamw_1cycle_bn128_1e3_3e4\n",
      "Loading checkpoint /content/drive/MyDrive/DaltonLens-Colab/DaltonLensPrivate/charts/pytorch/experiments/2022-Feb01-CR1/unet1_adamw_1cycle_bn128_1e3_3e4/checkpoint-00049.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_trainable_parameters(net): '18.3M'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected positive integer epochs, but got -9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-abcff6c5c8b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"=== [{i+1}/{len(configs)}] RUNNING CONFIG {config.name} ==\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mrun_xp_config\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozen_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a86dcab2e88e>\u001b[0m in \u001b[0;36mrun_xp_config\u001b[0;34m(xp, config, frozen_epochs, total_epochs, profiler)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trainable_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrozen_epochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozen_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-abcff6c5c8b9>\u001b[0m in \u001b[0;36mcreate_scheduler\u001b[0;34m(self, optimizer, frozen, steps_per_epoch, total_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmax_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_lr_frozen\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfrozen\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_lr_tune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneCycleLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, optimizer, max_lr, total_steps, epochs, steps_per_epoch, pct_start, anneal_strategy, cycle_momentum, base_momentum, max_momentum, div_factor, final_div_factor, three_phase, last_epoch, verbose)\u001b[0m\n\u001b[1;32m   1454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1456\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected positive integer epochs, but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected positive integer steps_per_epoch, but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected positive integer epochs, but got -9"
     ]
    }
   ],
   "source": [
    "\n",
    "class ConfigUnet1Adam(Config):\n",
    "    def __init__(self, name, max_lr_frozen, max_lr_tune, one_cycle: bool = True, *args, **kwargs):\n",
    "        super().__init__(name, *args, **kwargs)\n",
    "        self.one_cycle = one_cycle\n",
    "        self.max_lr_frozen = max_lr_frozen\n",
    "        self.max_lr_tune = max_lr_tune\n",
    "\n",
    "    def create_net(self): return cr.RegressionNet_Unet1()\n",
    "\n",
    "    def create_optimizer(self, net): \n",
    "        return optim.Adam([\n",
    "            {'params': net.encoder.parameters(), 'lr': self.max_lr_frozen[0] },\n",
    "            {'params': net.decoder.parameters(), 'lr': self.max_lr_frozen[1] }\n",
    "        ])\n",
    "\n",
    "    def create_scheduler(self, optimizer, frozen: bool, steps_per_epoch: int, total_epochs: int):\n",
    "        if not self.one_cycle:\n",
    "            return None\n",
    "        max_lr = self.max_lr_frozen if frozen else self.max_lr_tune\n",
    "        return torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, steps_per_epoch=steps_per_epoch, epochs=total_epochs)\n",
    "\n",
    "    def get_hyperparams(self):\n",
    "        return merge_dicts(super().get_hyperparams(), dict(\n",
    "            net='unet1',\n",
    "            opt='adam',\n",
    "            sched='1cycle' if self.one_cycle else 'none',\n",
    "            enc_lr_frozen=self.max_lr_frozen[0],\n",
    "            dec_lr_frozen=self.max_lr_frozen[1],\n",
    "            enc_lr_tune=self.max_lr_tune[0],\n",
    "            dec_lr_tune=self.max_lr_tune[1]\n",
    "        ))\n",
    "\n",
    "class ConfigUnet1AdamW(ConfigUnet1Adam):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def create_optimizer(self, net): \n",
    "        return optim.AdamW([\n",
    "            {'params': net.encoder.parameters(), 'lr': self.max_lr_frozen[0] },\n",
    "            {'params': net.decoder.parameters(), 'lr': self.max_lr_frozen[1] }\n",
    "        ])\n",
    "\n",
    "    def get_hyperparams(self):\n",
    "        return merge_dicts(super().get_hyperparams(), dict(opt='AdamW'))\n",
    "\n",
    "class ConfigUnet1SGD(ConfigUnet1Adam):\n",
    "    def __init__(self, name, momentum, *args, **kwargs):\n",
    "        super().__init__(name, *args, **kwargs)\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def create_optimizer(self, net):\n",
    "        return optim.SGD([\n",
    "            {'params': net.encoder.parameters(), 'lr': self.max_lr_frozen[0] },\n",
    "            {'params': net.decoder.parameters(), 'lr': self.max_lr_frozen[1] }\n",
    "        ], momentum=self.momentum)\n",
    "\n",
    "    def get_hyperparams(self):\n",
    "        return merge_dicts(super().get_hyperparams(), dict(opt='SGD'))\n",
    "\n",
    "configs = [\n",
    "    # TODO: test diffent batch sizes and lr with AdamW\n",
    "    # ConfigUnet1Adam('unet1_adam_3e4', max_lr_frozen=(1e-5, 3e-4), max_lr_tune=(1e-5, 3e-4), one_cycle=False),\n",
    "\n",
    "    # ConfigUnet1Adam('unet1_adam_1cycle_1e3', max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 3e-4)),\n",
    "    # ConfigUnet1Adam('unet1_adam_1cycle_3e4', max_lr_frozen=(1e-5, 3e-4), max_lr_tune=(1e-5, 1e-4)),\n",
    "    \n",
    "    # ConfigUnet1SGD('unet1_sgd_1cycle_1e3_09', max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 3e-4), momentum=0.9),\n",
    "    # ConfigUnet1SGD('unet1_sgd_1cycle_1e3_099', max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 3e-4), momentum=0.99),\n",
    "\n",
    "    # ConfigUnet1AdamW('unet1_adamw_1cycle_1e3_3e4', max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 3e-4)),\n",
    "    # ConfigUnet1AdamW('unet1_adamw_1cycle_3e4_1e4', max_lr_frozen=(1e-5, 3e-4), max_lr_tune=(1e-5, 1e-4)),\n",
    "    # ConfigUnet1AdamW('unet1_adamw_1cycle_3e4_3e4', max_lr_frozen=(1e-5, 3e-4), max_lr_tune=(1e-5, 3e-4)),\n",
    "    # ConfigUnet1AdamW('unet1_adamw_1cycle_1e3_1e3', max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 1e-3)),\n",
    "\n",
    "    # ConfigUnet1AdamW('unet1_adamw_1cycle_bn128_1e3_3e4', batch_size=128, max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 3e-4)),\n",
    "    # ConfigUnet1AdamW('unet1_adamw_1cycle_bn64_1e3_3e4', batch_size=64, max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 3e-4)),\n",
    "    ConfigUnet1AdamW('unet1_adamw_1cycle_bn32_1e3_3e4', batch_size=32, max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 3e-4)),\n",
    "    ConfigUnet1AdamW('unet1_adamw_1cycle_bn16_1e3_3e4', batch_size=16, max_lr_frozen=(1e-5, 1e-3), max_lr_tune=(1e-5, 3e-4)),\n",
    "\n",
    "    ConfigUnet1AdamW('unet1_adamw_1cycle_bn64_3e4_3e4', batch_size=64, max_lr_frozen=(1e-5, 3e-4), max_lr_tune=(1e-5, 3e-4)),\n",
    "    ConfigUnet1AdamW('unet1_adamw_1cycle_bn64_5e3_1e3', batch_size=64, max_lr_frozen=(1e-5, 5e-3), max_lr_tune=(1e-5, 1e-3)),\n",
    "]\n",
    "\n",
    "# with torch.profiler.profile(\n",
    "#     activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "#     schedule=torch.profiler.schedule(\n",
    "#         wait=2,\n",
    "#         warmup=2,\n",
    "#         active=6,\n",
    "#         repeat=1),\n",
    "#     on_trace_ready=torch.profiler.tensorboard_trace_handler(utils.default_output_dir / 'profiler'),\n",
    "#     with_stack=True\n",
    "# ) as profiler:\n",
    "#     xp = Experiment(\"2022-Feb01-CR1-Profiler\", utils.default_output_dir, clear_previous_results=True, clear_top_folder=True)\n",
    "#     print (f\"=== RUNNING PROFILING CONFIG {configs[0].name} ==\")\n",
    "#     run_xp_config (xp, configs[0], frozen_epochs=1, total_epochs=1, profiler=profiler)\n",
    "\n",
    "logs_root_dir = utils.default_output_dir\n",
    "xp = Experiment(\"2022-Feb01-CR1\", logs_root_dir, clear_previous_results=True, clear_top_folder=False)\n",
    "for i, config in enumerate(configs):\n",
    "    print (f\"=== [{i+1}/{len(configs)}] RUNNING CONFIG {config.name} ==\")\n",
    "    run_xp_config (xp, config, frozen_epochs=40, total_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_specific_checkpoint (name):\n",
    "    checkpoint = torch.load(xp.log_path / name, map_location=device)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# load_specific_checkpoint (\"checkpoint-00701.pt\")\n",
    "# torch.save (net, \"regression_unet_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input, labels, _ = next(iter(train_dataloader))\n",
    "    input, labels = [x.to(device) for x in [input, labels]]\n",
    "    output = net(input.to(device))\n",
    "    #clear_output(wait=True)\n",
    "    plt.figure()\n",
    "    plt.imshow (preprocessor.denormalize_and_clip_as_numpy(output[0]))\n",
    "    plt.figure()\n",
    "    plt.imshow (preprocessor.denormalize_and_clip_as_numpy(labels[0]))\n",
    "    plt.figure()\n",
    "    plt.imshow (preprocessor.denormalize_and_clip_as_numpy(input[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the google colab VM\n",
    "utils.stop_google_colab_vm ()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "016a8caea8d0bcbcb0f585ee40a090ed0405075dd6b5b528270a2e6b8c256090"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
